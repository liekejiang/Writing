## Introduction
介绍了近年来（15年）神经网络的发展情况，指出由于嵌入式和移动平台的兴起，比起效果上纯粹的数字提升，参数数量以及效率的重要性如今更加值得关注。本文提出的**Inception**就着重关注这一方面。

自从VGG出现之后，大家都在研究如何把网络做的更深并利用深度去提升效果。而在**Inception**中，**DEEP**除了指网络结构的深度外，还代表了本文提出的具有全新组织模式的**Inception Module** 。

## Related Work
文章指出自**LeNet**以来，大部分的结构上的创新都是卷机层，池化层和最新的BN层的各种组合，但是这些组合也创造出了迄今为止图象识别的最佳记录。而在大数据集的处理方面，增加卷机层和每层filters的数量，以及**dropout**成为防止过拟合的主流方法。

尽管最大池化层会造成空间信息方面的损失，但是这些网络结构还是成功地应用于定位，物体识别和人体姿势预测。

从神经科学的的研究成功--灵长类大脑视觉皮层模型收到启发，有人使用了一系列大小不一的滤波器去处理多尺度模型，我们的**Inception**也从中获得灵感。但是在**Inception**模型中，所有**filter**都是经过多次学习得到的（感觉是废话）。

（重点）Network-in-Network[12]被提出用于增加网络模型的表达能力，我们发现其中使用的瓶颈层`1*1的卷机层，在DenseNet，ResNet中都有使用`增加了模型的深度（然后抢了过来）。在我们的**Inception**模型中使用瓶颈层有两个目的：①是增加网络的深度②是在没有太多惩罚的情况下增加网络宽度。

（重点）最后介绍了R-CNN中使用的RPN和分类器结合的方法，并说明**Incetion**在此基础上使用其他方法诸如multi-box prediction提高bounding box的召回率以及其他提高分类成功率的办法，

## Motivation and High Level Considerations
根据作者团队的经验，提高深度神经网络效果最直接的方法就是增加**深度**和**宽度**， 这里的**宽度**代表`原文：the number of units at each level` 我理解为增加filter的数量，也就是输出维度。这种方法尤其是在有大量标签数据集的情况下简单又安全，但是有两个主要的缺点：

1. 网络大小的增加会带来训练参数的几何倍的增长，使得网络容易过拟合。这一点在数据集有限的情况下十分严重，然而专业的数据集费时费力很难获得。
2. 训练参数的几何倍的增长使得训练所需算力也是十分巨大的。若新加入的卷机层或者维度的权重趋于0，代表这部分结构是冗余的，降低了算力和参数的有效利用率

一个解决这些问题的基础办法是将全连接层用稀疏连接层取代（？），有论文已经提出，在条件严格的情况下，数据集的概率分布可以被一个足够大的稀疏深度网络所表达，然后最优网络拓扑就可以通过分析前一层的激活函数和聚类神经元有高度相关性的输出的相关性统计变量一层层的构建出来（我也不懂）。 这一论述和神经科学中的`Hebbian Principle`有异曲同工之妙。

然而由于当今计算机硬件架构对非均匀稀疏数据结构并不友好，采用该方法可能并不能带来任何的好处。因此现在研究又转回了全连接层以便进一步优化多线程计算。那么有没有折中的办法呢，文章提出在filter上想办法引入稀疏，然后在硬件计算上保持密集矩阵的计算。（后略）

